---
published: true
title: Vietnamese news corpus
---
## 2 millions news corpus for Vietnamese NLP task

MongoDB (all information: author, images, cover, ....): ~6GB uncompressed
[dantri demo](https://github.com/Avi197/Avi197.github.io/blob/master/news%20corpus%20demo/mongodb%20news%20corpus%20demo%20dantri)
[Download](https://drive.google.com/file/d/1gTFdON-3DFL1HJ-01VXfmmPUzmdNPLzX/view?usp=sharing)

MongoDB demo
![dantri demo.png]({{site.baseurl}}/_posts/dantri demo.png)
![detail demo.png]({{site.baseurl}}/_posts/detail demo.png)


title and description only (classification): ~500MB uncompress
[Download](https://drive.google.com/file/d/1tavVhYYqMwdbH3fnqNcNCXUMO0IJO2-1/view?usp=sharing)

Raw text
![non tokenized demo.png]({{site.baseurl}}/_posts/non tokenized demo.png)
Tokenized text
![tokenized demo.png]({{site.baseurl}}/_posts/tokenized demo.png)

Title, description, content tokenized (raw text): ~5GB uncompressed, ~1GB compressed
[Download](https://drive.google.com/file/d/1pXeX8YFOE1BRpKusAYFdBmjO6X9IH-g2/view?usp=sharing)


There is a bigger news corpus by binvq, contain around 14 millions news, use that one if you need a lot of data
[Binhvq news corpus](https://github.com/binhvq/news-corpus)
