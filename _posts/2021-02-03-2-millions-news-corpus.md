---
published: false
---
## 2 millions news corpus for Vietnamese NLP task

MongoDB (all information: author, images, cover, ....): ~6GB uncompressed
[dantri demo](https://github.com/Avi197/Avi197.github.io/blob/master/news%20corpus%20demo/mongodb%20news%20corpus%20demo%20dantri)
[Download](https://drive.google.com/file/d/1gTFdON-3DFL1HJ-01VXfmmPUzmdNPLzX/view?usp=sharing)


title and description only (classification): ~500MB uncompress
[Download](https://drive.google.com/file/d/1tavVhYYqMwdbH3fnqNcNCXUMO0IJO2-1/view?usp=sharing)


Title, description, content tokenized (raw text): ~5GB uncompressed, ~1GB compressed
[Download](https://drive.google.com/file/d/1pXeX8YFOE1BRpKusAYFdBmjO6X9IH-g2/view?usp=sharing)


There is a bigger news corpus by binvq, contain around 14 millions news, use that one if you need a lot of data
[Binhvq news corpus](https://github.com/binhvq/news-corpus)

